{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-01T13:44:04.014284Z",
     "start_time": "2018-04-01T13:44:03.293233Z"
    },
    "scrolled": false
   },
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "- Concatenate files into one dataframe for each\n",
    "- Calculate FPTS\n",
    "- Resolve name inconsistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-20T13:44:38.972185Z",
     "start_time": "2018-05-20T13:44:36.677930Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from datetime        import datetime\n",
    "from urllib.request  import urlopen\n",
    "from bs4             import BeautifulSoup\n",
    "\n",
    "pd.set_option(\"display.max_columns\",40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-20T13:49:35.851524Z",
     "start_time": "2018-05-20T13:49:35.847140Z"
    }
   },
   "outputs": [],
   "source": [
    "cwd = os.getcwd().replace('/notebooks','')\n",
    "data_dir = os.path.join(cwd, 'data')\n",
    "season = '2014-15'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Game Data from Basketball Reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-20T13:49:36.882594Z",
     "start_time": "2018-05-20T13:49:36.863917Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Concatenate all csv files under a directory\n",
    "def csv_concatenate(folder_path):\n",
    "    files = glob.glob(folder_path + \"/*.csv\")\n",
    "    df_list = []\n",
    "    for file in tqdm(files):\n",
    "        df_list.append(pd.read_csv(file, parse_dates=True, infer_datetime_format=True))\n",
    "    #Fill nan with 0s as some values are empty for percentage points\n",
    "    df = pd.concat(df_list).fillna(0).reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-20T13:49:37.561435Z",
     "start_time": "2018-05-20T13:49:37.524382Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def calculate_FPTS(df):\n",
    "    #Scoring rules based on https://www.draftkings.co.uk/help/rules/4\n",
    "    multipliers = {'PTS':1, '3P': 0.5, 'TRB':1.25, 'AST':1.5, 'STL':2, 'BLK':2, 'TOV':-0.5}\n",
    "\n",
    "    fpts_list = []\n",
    "    \n",
    "    for i in tqdm(range(df.shape[0])):\n",
    "        fpts = 0\n",
    "        doubles_count = 0\n",
    "        for stat, multiplier in multipliers.items():\n",
    "            if stat in ['PTS', 'TRB', 'AST', 'STL', 'BLK']:\n",
    "                if df.loc[i, stat] >= 10:\n",
    "                    doubles_count += 1\n",
    "            fpts += df.loc[i, stat]*multiplier\n",
    "        \n",
    "        if doubles_count >= 2:\n",
    "            fpts += 1.5\n",
    "            \n",
    "        if doubles_count >= 3:\n",
    "            fpts += 3\n",
    "            \n",
    "        fpts_list.append(fpts) \n",
    "        \n",
    "    return fpts_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-20T13:49:38.328605Z",
     "start_time": "2018-05-20T13:49:38.304980Z"
    }
   },
   "outputs": [],
   "source": [
    "def add_doubles(df):\n",
    "    \n",
    "    dd = [0 for i in range(df.shape[0])]\n",
    "    td = [0 for i in range(df.shape[0])]\n",
    "    \n",
    "    for i in tqdm(range(df.shape[0])):\n",
    "        doubles_count = 0\n",
    "        check_doubles = ['PTS','TRB', 'AST', 'STL', 'BLK']\n",
    "        \n",
    "        for stat in check_doubles:\n",
    "            if df.loc[i, stat] >= 10:\n",
    "                doubles_count += 1\n",
    "        \n",
    "        if doubles_count >= 2:\n",
    "            dd[i] = 1\n",
    "        if doubles_count >= 3:\n",
    "            td[i] = 1\n",
    "   \n",
    "    df['DD'] = dd\n",
    "    df['TD'] = td"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-20T13:50:05.108403Z",
     "start_time": "2018-05-20T13:49:39.058906Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_games = csv_concatenate(os.path.join(data_dir, 'Games', season))\n",
    "df_games['FPTS'] = calculate_FPTS(df_games)\n",
    "add_doubles(df_games)\n",
    "columns = ['Name', 'Date', 'Team',  'FPTS', 'Home','W', 'W_PTS', 'L', 'L_PTS', 'MP',\n",
    "           'FG', 'FGA', 'FG_perc', '3P', '3PA', '3P_perc', 'FT', 'FTA', 'FT_perc',\n",
    "           'ORB', 'DRB', 'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF', 'PTS', 'DD', 'TD', \n",
    "           'USG_perc','DRtg','ORtg','AST_perc','DRB_perc','ORB_perc','BLK_perc','TOV_perc','STL_perc','eFG_perc']\n",
    "df_games = df_games.loc[:, columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T16:06:50.196829Z",
     "start_time": "2018-03-13T16:06:50.193649Z"
    },
    "collapsed": true
   },
   "source": [
    "### Name Standardization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-20T13:50:06.958557Z",
     "start_time": "2018-05-20T13:50:06.848751Z"
    }
   },
   "outputs": [],
   "source": [
    "def parse_name(term):\n",
    "    search_url = 'https://www.basketball-reference.com/search/search.fcgi?hint=&search={term}&pid=&idx='\n",
    "    name_url = search_url.format(term=term.replace(' ','+'))\n",
    "    soup = BeautifulSoup(urlopen(name_url),'html5lib')\n",
    "    \n",
    "    #Check if there is ambiguity in the name\n",
    "    if soup.find('h1').get_text() != 'Search Results':\n",
    "        return soup.find('h1').get_text()\n",
    "    \n",
    "    \n",
    "    elif (soup.find('div', id='players', class_='current') == None):\n",
    "        if (len(term.split(' ')) > 2) or ('.' in term):\n",
    "            #Parse again without periods and with first two names\n",
    "            new_term = ' '.join(term.replace('.','').split(' ')[:2])\n",
    "            return parse_name(new_term)\n",
    "        else:\n",
    "            return np.nan\n",
    "                    \n",
    "    else:        \n",
    "        items = soup.find('div', id='players', class_='current').find_all('div', class_='search-item-name')\n",
    "        current_years = (int(season[:4]), int(season[:2]+season[-2:]))\n",
    "        candidates = []\n",
    "        \n",
    "        for item in items:\n",
    "            name = item.find('a').get_text()\n",
    "\n",
    "            if '(' not in name:\n",
    "                candidates.append(name)\n",
    "\n",
    "            else:\n",
    "                career = name[name.find('(')+1:name.find(')')].split('-')\n",
    "                if len(career) == 1:\n",
    "                    if int(career[0]) in current_years:\n",
    "                        candidates.append(name[:name.find(' (')])\n",
    "                else:\n",
    "                    start = int(career[0])\n",
    "                    end = int(career[1])\n",
    "\n",
    "                    for year in current_years: \n",
    "                        if year in range(start, end+1):\n",
    "                            candidates.append(name[:name.find(' (')])\n",
    "                            break\n",
    "                            \n",
    "        if len(candidates) != 0:\n",
    "            for candidate in candidates:\n",
    "                if term in candidate:\n",
    "                    return candidate\n",
    "            return candidates[0]\n",
    "        \n",
    "        else:\n",
    "            return np.nan\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-20T13:50:07.980726Z",
     "start_time": "2018-05-20T13:50:07.957078Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_standard_names(df):\n",
    "    names = list(set(df['Name']))\n",
    "    standard_names = []\n",
    "    \n",
    "    for i, name in enumerate(names):\n",
    "        standard_name = parse_name(name)\n",
    "        print(i, standard_name)\n",
    "        standard_names.append(standard_name)\n",
    "        time.sleep(1)\n",
    "    \n",
    "    return standard_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-20T13:50:08.517798Z",
     "start_time": "2018-05-20T13:50:08.495810Z"
    }
   },
   "outputs": [],
   "source": [
    "def standardize_names(df, standard_names):\n",
    "    names = list(set(df['Name']))\n",
    "    \n",
    "    df = df.dropna().reset_index(drop=True)\n",
    "    \n",
    "    diff = [name for name in names if name not in standard_names]\n",
    "    print('{} items are standardized ...'.format(len(diff)))\n",
    "    \n",
    "    names_conversion = {}\n",
    "    \n",
    "    for name in tqdm(names):\n",
    "        if name in diff:\n",
    "            names_conversion[name] = parse_name(name)\n",
    "            time.sleep(1)\n",
    "    \n",
    "    for i in range(df.shape[0]):\n",
    "        name = df.loc[i,'Name']\n",
    "        if name in names_conversion.keys():\n",
    "            df.loc[i,'Name'] = names_conversion[name]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-20T13:50:09.214881Z",
     "start_time": "2018-05-20T13:50:09.204648Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_name_pos(df):\n",
    "    \n",
    "    name_pos = {}\n",
    "    \n",
    "    for name in set(df['Name']):\n",
    "        pos = df.loc[(df['Name']==name) & (df['Pos']!=0), 'Pos'].mode()\n",
    "        if len(pos) != 0:\n",
    "            name_pos[name] = pos[0]\n",
    "    \n",
    "    return name_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-20T13:50:09.856455Z",
     "start_time": "2018-05-20T13:50:09.845943Z"
    }
   },
   "outputs": [],
   "source": [
    "def fill_pos(df):\n",
    "    \n",
    "    name_pos = generate_name_pos(df)\n",
    "    \n",
    "    for i in tqdm(range(df.shape[0])):\n",
    "        if df.loc[i, 'Pos'] == 0:\n",
    "            name = df.loc[i, 'Name']\n",
    "            if name in name_pos.keys():\n",
    "                #print(df.loc[i,'Date'], name)\n",
    "                df.loc[i, 'Pos'] = name_pos[name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-20T13:50:10.876040Z",
     "start_time": "2018-05-20T13:50:10.503414Z"
    }
   },
   "outputs": [],
   "source": [
    "df_salary = csv_concatenate(os.path.join(data_dir, 'DKSalary', season))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-20T14:21:46.720958Z",
     "start_time": "2018-05-20T13:50:34.146667Z"
    }
   },
   "outputs": [],
   "source": [
    "#Takes about 30 mins\n",
    "\n",
    "standard_names = generate_standard_names(df_salary)\n",
    "with open(os.path.join(data_dir, 'standard_names','{}.npy'.format(season)), \"wb\") as fp:\n",
    "    pickle.dump(standard_names, fp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-20T14:27:57.515054Z",
     "start_time": "2018-05-20T14:27:57.511577Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(data_dir, 'standard_names', '{}.npy'.format(season)), \"rb\") as fp:\n",
    "    standard_names = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-20T14:28:56.025613Z",
     "start_time": "2018-05-20T14:27:58.556046Z"
    }
   },
   "outputs": [],
   "source": [
    "standardize_names(df_salary, standard_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-20T14:29:32.639099Z",
     "start_time": "2018-05-20T14:28:56.225213Z"
    }
   },
   "outputs": [],
   "source": [
    "standardize_names(df_games, standard_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-20T14:29:36.114196Z",
     "start_time": "2018-05-20T14:29:32.840370Z"
    }
   },
   "outputs": [],
   "source": [
    "fill_pos(df_salary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-20T14:29:36.581536Z",
     "start_time": "2018-05-20T14:29:36.318127Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.merge(df_salary.drop('Team', axis=1), df_games, on=['Name', 'Date'], how='inner')\n",
    "df = df[df['Pos']!=0].sort_values(by=['Date','Team']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-20T14:29:37.032596Z",
     "start_time": "2018-05-20T14:29:36.958468Z"
    }
   },
   "outputs": [],
   "source": [
    "# Add Value variable\n",
    "df['Value'] = df['FPTS']/(df['Salary']/1000)\n",
    "df['Value'] = df['Value'].replace(np.inf, 0).replace(-np.inf, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-20T14:29:38.331695Z",
     "start_time": "2018-05-20T14:29:37.408112Z"
    }
   },
   "outputs": [],
   "source": [
    "columns = ['Name', 'Pos', 'Date', 'Team', 'FPTS', 'Value', 'Home', 'W', 'W_PTS', 'L', 'L_PTS', 'MP',\n",
    "           'FG', 'FGA', 'FG_perc', '3P', '3PA', '3P_perc', 'FT', 'FTA', 'FT_perc', \n",
    "           'ORB', 'DRB', 'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF', 'PTS', 'DD', 'TD', \n",
    "           'USG_perc','DRtg','ORtg','AST_perc','DRB_perc','ORB_perc','BLK_perc','TOV_perc','STL_perc','eFG_perc']\n",
    "\n",
    "columns = columns[:3] + ['Salary', 'Starter'] + columns[3:]\n",
    "\n",
    "df = df.loc[:, columns]\n",
    "\n",
    "df.to_csv(os.path.join(data_dir, 'Dataframes', 'clean','df_{}.csv'.format(season)), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
