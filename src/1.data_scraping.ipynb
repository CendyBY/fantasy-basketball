{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T05:53:12.252482Z",
     "start_time": "2018-03-31T05:53:12.248119Z"
    }
   },
   "source": [
    "### Data Scraping\n",
    "Scrape data for each season for boxscores and DraftKings fantasy salary information from Basketball-Reference and RotoGuru. Check existing files and create directories automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T20:27:06.987259Z",
     "start_time": "2019-05-25T20:27:06.289012Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "from constants import DATA_DIR, SEASON_DATES, SECONDS_SLEEP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T20:37:52.027084Z",
     "start_time": "2019-05-25T20:37:51.983762Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class DataScraper():\n",
    "    # Scraping Historical Game Data from Basketball-Reference.com\n",
    "    def get_boxscores(self, season, date_list):\n",
    "        url_parent = \"https://www.basketball-reference.com\"\n",
    "        url_boxscore = \"https://www.basketball-reference.com/boxscores/?month={month}&day={day}&year={year}\"\n",
    "\n",
    "        print(\"Scraping boxscores from the {} regular season\".format(season))\n",
    "        \n",
    "        for date in tqdm(date_list):\n",
    "            # BeautifulSoup object for a list of boxscores on a given day\n",
    "            url_summaries = url_boxscore.format(month=date[4:6],day=date[6:8],year=date[0:4])\n",
    "            soup_summaries = BeautifulSoup(urlopen(url_summaries),'lxml')\n",
    "            games = soup_summaries.find_all('div',class_='game_summary expanded nohover')\n",
    "\n",
    "            for game in games:\n",
    "                summary = {}\n",
    "\n",
    "                host = game.find_all('table')[1].find_all('a')[1]['href'][7:10]\n",
    "\n",
    "                winner = game.find('tr',class_='winner').find_all('td')\n",
    "                loser = game.find('tr',class_='loser').find_all('td')\n",
    "\n",
    "                summary['winner'] = [winner[0].find('a')['href'][7:10],int(winner[1].get_text())]\n",
    "                summary['loser'] = [loser[0].find('a')['href'][7:10],int(loser[1].get_text())]\n",
    "\n",
    "\n",
    "                url_game = url_parent+game.find('a',text='Box Score')['href']\n",
    "                soup_game = BeautifulSoup(urlopen(url_game),'lxml')\n",
    "\n",
    "\n",
    "                tables = soup_game.find_all('table',limit=4)[2:]\n",
    "\n",
    "                columns_basic = [th.get_text() for th in tables[0].find('thead').find_all('tr')[1].find_all('th')][1:]\n",
    "                columns_advanced = [th.get_text() for th in tables[1].find('thead').find_all('tr')[1].find_all('th')][2:]\n",
    "\n",
    "                game_columns = ['Name','Date','Team','Home','W','W_PTS','L','L_PTS']\n",
    "                column_headers = game_columns + columns_basic + columns_advanced\n",
    "\n",
    "                teams = ['winner','loser']\n",
    "                basic_stat_template = 'box_{team}_basic'\n",
    "                advanced_stat_template = 'box_{team}_advanced'\n",
    "\n",
    "                for team in teams:\n",
    "\n",
    "                    if summary[team][0] == host:\n",
    "                        home = 1\n",
    "                    else:\n",
    "                        home = 0\n",
    "\n",
    "                    basic_stat = basic_stat_template.format(team=summary[team][0].lower())\n",
    "                    advanced_stat = advanced_stat_template.format(team=summary[team][0].lower())\n",
    "\n",
    "                    game_data = [date, summary[team][0], home,summary['winner'][0],\n",
    "                                 summary['winner'][1], summary['loser'][0],summary['loser'][1]]\n",
    "\n",
    "                    data_basic = soup_game.find('table',id=basic_stat).find('tbody').find_all('tr',class_=None)\n",
    "                    data_advanced = soup_game.find('table',id=advanced_stat).find('tbody').find_all('tr',class_=None)\n",
    "\n",
    "                    n = len(data_basic)\n",
    "\n",
    "                    player_names = [data_basic[i].find('a').get_text() for i in range(n)]\n",
    "\n",
    "                    player_data = []\n",
    "                    injury_keywords = ['Did Not Dress', 'Not With Team']\n",
    "\n",
    "                    for i in range(n):\n",
    "                        if data_basic[i].find('td').get_text() not in injury_keywords:\n",
    "                            data = [player_names[i]] + game_data + \\\n",
    "                                   [td.get_text() for td in data_basic[i].find_all('td')] + \\\n",
    "                                   [td.get_text() for td in data_advanced[i].find_all('td')[1:]]\n",
    "\n",
    "                            player_data.append(data)\n",
    "\n",
    "                    df = pd.DataFrame(player_data,columns=column_headers)\n",
    "                    df.columns = df.columns.str.replace('%','_perc').str.replace('/','')\n",
    "                    df = df.fillna(0)\n",
    "                    df.loc[:,'FG':'+-'] = df.loc[:,'FG':'+-'].apply(pd.to_numeric)\n",
    "                    df['MP'] = [0.00 if ':' not in t else round(int(t.split(':')[0])+int(t.split(':')[1])/60, 2) for t in df['MP']] \n",
    "                    df.to_csv(os.path.join(*[DATA_DIR, 'Boxscores', season, date+'-'+summary[team][0]+'.csv']), index=False)\n",
    "\n",
    "                time.sleep(SECONDS_SLEEP)\n",
    "        return None\n",
    "\n",
    "    # Scraping DraftKings salary data from RotoGuru.com\n",
    "    def get_fantasy_salary(self, season, date_list):\n",
    "        url_roto = \"http://rotoguru1.com/cgi-bin/hyday.pl?mon={month}&day={day}&year={year}&game=dk\"  \n",
    "        print(\"Scraping salary information from the {} regular season\".format(season))\n",
    "        \n",
    "        for date in tqdm(date_list):\n",
    "            print(date)\n",
    "            teams, positions, players, starters, salaries = [], [], [], [], []\n",
    "\n",
    "            url_date = url_roto.format(month=date[4:6],day=date[6:8],year=date[0:4])\n",
    "            soup = BeautifulSoup(urlopen(url_date),'lxml')\n",
    "\n",
    "            #Check if there were any games on a given date\n",
    "            soup_table = soup.find('body').find('table', border=\"0\", cellspacing=\"5\")\n",
    "\n",
    "            soup_rows = soup_table.find_all('tr')\n",
    "\n",
    "            for row in soup_rows:\n",
    "                if row.find('td').has_attr('colspan') == False:\n",
    "                    if row.find('a').get_text() != '':\n",
    "\n",
    "                        position = row.find_all('td')[0].get_text()\n",
    "\n",
    "                        player_tmp = row.find('a').get_text().split(\", \")\n",
    "                        player = player_tmp[1] + ' ' + player_tmp[0]\n",
    "\n",
    "                        starter_tmp = row.find_all('td')[1].get_text()\n",
    "\n",
    "                        if '^' in starter_tmp:\n",
    "                            starter = 1\n",
    "                        else:\n",
    "                            starter =0\n",
    "\n",
    "                        salary_tmp = row.find_all('td')[3].get_text()\n",
    "                        salary = re.sub('[$,]', '', salary_tmp)\n",
    "\n",
    "                        team = row.find_all('td')[4].get_text()\n",
    "\n",
    "                        positions.append(position)\n",
    "                        players.append(player)\n",
    "                        starters.append(starter)\n",
    "                        salaries.append(salary)\n",
    "                        teams.append(team)\n",
    "\n",
    "            df = pd.DataFrame({'Date': [date for i in range(len(players))], \n",
    "                               'Team': [team.upper() for team in teams],\n",
    "                               'Starter': starters,\n",
    "                               'Pos': positions,\n",
    "                               'Name': players,\n",
    "                               'Salary': salaries})\n",
    "\n",
    "            df = df.loc[:,['Date','Team','Pos','Name','Starter','Salary']]\n",
    "            df.to_csv(os.path.join(DATA_DIR, 'DKSalary', season, 'salary_'+date+'.csv'), index=False)\n",
    "\n",
    "            time.sleep(SECONDS_SLEEP)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T20:59:26.409261Z",
     "start_time": "2019-05-25T20:37:59.724959Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping salary information from the 2014-15 regular season\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "497fb112bd604075aa144790a77529b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=167), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20141031\n",
      "20141101\n",
      "20141102\n",
      "20141103\n",
      "20141104\n",
      "20141105\n",
      "20141106\n",
      "20141107\n",
      "20141108\n",
      "20141109\n",
      "20141110\n",
      "20141111\n",
      "20141112\n",
      "20141113\n",
      "20141114\n",
      "20141115\n",
      "20141116\n",
      "20141117\n",
      "20141118\n",
      "20141119\n",
      "20141120\n",
      "20141121\n",
      "20141122\n",
      "20141123\n",
      "20141124\n",
      "20141125\n",
      "20141126\n",
      "20141127\n",
      "20141128\n",
      "20141129\n",
      "20141130\n",
      "20141201\n",
      "20141202\n",
      "20141203\n",
      "20141204\n",
      "20141205\n",
      "20141206\n",
      "20141207\n",
      "20141208\n",
      "20141209\n",
      "20141210\n",
      "20141211\n",
      "20141212\n",
      "20141213\n",
      "20141214\n",
      "20141215\n",
      "20141216\n",
      "20141217\n",
      "20141218\n",
      "20141219\n",
      "20141220\n",
      "20141221\n",
      "20141222\n",
      "20141223\n",
      "20141224\n",
      "20141225\n",
      "20141226\n",
      "20141227\n",
      "20141228\n",
      "20141229\n",
      "20141230\n",
      "20141231\n",
      "20150101\n",
      "20150102\n",
      "20150103\n",
      "20150104\n",
      "20150105\n",
      "20150106\n",
      "20150107\n",
      "20150108\n",
      "20150109\n",
      "20150110\n",
      "20150111\n",
      "20150112\n",
      "20150113\n",
      "20150114\n",
      "20150115\n",
      "20150116\n",
      "20150117\n",
      "20150118\n",
      "20150119\n",
      "20150120\n",
      "20150121\n",
      "20150122\n",
      "20150123\n",
      "20150124\n",
      "20150125\n",
      "20150126\n",
      "20150127\n",
      "20150128\n",
      "20150129\n",
      "20150130\n",
      "20150131\n",
      "20150201\n",
      "20150202\n",
      "20150203\n",
      "20150204\n",
      "20150205\n",
      "20150206\n",
      "20150207\n",
      "20150208\n",
      "20150209\n",
      "20150210\n",
      "20150211\n",
      "20150212\n",
      "20150213\n",
      "20150214\n",
      "20150215\n",
      "20150216\n",
      "20150217\n",
      "20150218\n",
      "20150219\n",
      "20150220\n",
      "20150221\n",
      "20150222\n",
      "20150223\n",
      "20150224\n",
      "20150225\n",
      "20150226\n",
      "20150227\n",
      "20150228\n",
      "20150301\n",
      "20150302\n",
      "20150303\n",
      "20150304\n",
      "20150305\n",
      "20150306\n",
      "20150307\n",
      "20150308\n",
      "20150309\n",
      "20150310\n",
      "20150311\n",
      "20150312\n",
      "20150313\n",
      "20150314\n",
      "20150315\n",
      "20150316\n",
      "20150317\n",
      "20150318\n",
      "20150319\n",
      "20150320\n",
      "20150321\n",
      "20150322\n",
      "20150323\n",
      "20150324\n",
      "20150325\n",
      "20150326\n",
      "20150327\n",
      "20150328\n",
      "20150329\n",
      "20150330\n",
      "20150331\n",
      "20150401\n",
      "20150402\n",
      "20150403\n",
      "20150404\n",
      "20150405\n",
      "20150406\n",
      "20150407\n",
      "20150408\n",
      "20150409\n",
      "20150410\n",
      "20150411\n",
      "20150412\n",
      "20150413\n",
      "20150414\n",
      "20150415\n",
      "\n",
      "Scraping salary information from the 2015-16 regular season\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f8aee3b2bf3416eb48e031f75f9e447",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=170), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20151027\n",
      "20151028\n",
      "20151029\n",
      "20151030\n",
      "20151031\n",
      "20151101\n",
      "20151102\n",
      "20151103\n",
      "20151104\n",
      "20151105\n",
      "20151106\n",
      "20151107\n",
      "20151108\n",
      "20151109\n",
      "20151110\n",
      "20151111\n",
      "20151112\n",
      "20151113\n",
      "20151114\n",
      "20151115\n",
      "20151116\n",
      "20151117\n",
      "20151118\n",
      "20151119\n",
      "20151120\n",
      "20151121\n",
      "20151122\n",
      "20151123\n",
      "20151124\n",
      "20151125\n",
      "20151126\n",
      "20151127\n",
      "20151128\n",
      "20151129\n",
      "20151130\n",
      "20151201\n",
      "20151202\n",
      "20151203\n",
      "20151204\n",
      "20151205\n",
      "20151206\n",
      "20151207\n",
      "20151208\n",
      "20151209\n",
      "20151210\n",
      "20151211\n",
      "20151212\n",
      "20151213\n",
      "20151214\n",
      "20151215\n",
      "20151216\n",
      "20151217\n",
      "20151218\n",
      "20151219\n",
      "20151220\n",
      "20151221\n",
      "20151222\n",
      "20151223\n",
      "20151224\n",
      "20151225\n",
      "20151226\n",
      "20151227\n",
      "20151228\n",
      "20151229\n",
      "20151230\n",
      "20151231\n",
      "20160101\n",
      "20160102\n",
      "20160103\n",
      "20160104\n",
      "20160105\n",
      "20160106\n",
      "20160107\n",
      "20160108\n",
      "20160109\n",
      "20160110\n",
      "20160111\n",
      "20160112\n",
      "20160113\n",
      "20160114\n",
      "20160115\n",
      "20160116\n",
      "20160117\n",
      "20160118\n",
      "20160119\n",
      "20160120\n",
      "20160121\n",
      "20160122\n",
      "20160123\n",
      "20160124\n",
      "20160125\n",
      "20160126\n",
      "20160127\n",
      "20160128\n",
      "20160129\n",
      "20160130\n",
      "20160131\n",
      "20160201\n",
      "20160202\n",
      "20160203\n",
      "20160204\n",
      "20160205\n",
      "20160206\n",
      "20160207\n",
      "20160208\n",
      "20160209\n",
      "20160210\n",
      "20160211\n",
      "20160212\n",
      "20160213\n",
      "20160214\n",
      "20160215\n",
      "20160216\n",
      "20160217\n",
      "20160218\n",
      "20160219\n",
      "20160220\n",
      "20160221\n",
      "20160222\n",
      "20160223\n",
      "20160224\n",
      "20160225\n",
      "20160226\n",
      "20160227\n",
      "20160228\n",
      "20160229\n",
      "20160301\n",
      "20160302\n",
      "20160303\n",
      "20160304\n",
      "20160305\n",
      "20160306\n",
      "20160307\n",
      "20160308\n",
      "20160309\n",
      "20160310\n",
      "20160311\n",
      "20160312\n",
      "20160313\n",
      "20160314\n",
      "20160315\n",
      "20160316\n",
      "20160317\n",
      "20160318\n",
      "20160319\n",
      "20160320\n",
      "20160321\n",
      "20160322\n",
      "20160323\n",
      "20160324\n",
      "20160325\n",
      "20160326\n",
      "20160327\n",
      "20160328\n",
      "20160329\n",
      "20160330\n",
      "20160331\n",
      "20160401\n",
      "20160402\n",
      "20160403\n",
      "20160404\n",
      "20160405\n",
      "20160406\n",
      "20160407\n",
      "20160408\n",
      "20160409\n",
      "20160410\n",
      "20160411\n",
      "20160412\n",
      "20160413\n",
      "\n",
      "Scraping salary information from the 2016-17 regular season\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e3c47cfcaf34aae84b85a25d7d0142a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=170), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20161025\n",
      "20161026\n",
      "20161027\n",
      "20161028\n",
      "20161029\n",
      "20161030\n",
      "20161031\n",
      "20161101\n",
      "20161102\n",
      "20161103\n",
      "20161104\n",
      "20161105\n",
      "20161106\n",
      "20161107\n",
      "20161108\n",
      "20161109\n",
      "20161110\n",
      "20161111\n",
      "20161112\n",
      "20161113\n",
      "20161114\n",
      "20161115\n",
      "20161116\n",
      "20161117\n",
      "20161118\n",
      "20161119\n",
      "20161120\n",
      "20161121\n",
      "20161122\n",
      "20161123\n",
      "20161124\n",
      "20161125\n",
      "20161126\n",
      "20161127\n",
      "20161128\n",
      "20161129\n",
      "20161130\n",
      "20161201\n",
      "20161202\n",
      "20161203\n",
      "20161204\n",
      "20161205\n",
      "20161206\n",
      "20161207\n",
      "20161208\n",
      "20161209\n",
      "20161210\n",
      "20161211\n",
      "20161212\n",
      "20161213\n",
      "20161214\n",
      "20161215\n",
      "20161216\n",
      "20161217\n",
      "20161218\n",
      "20161219\n",
      "20161220\n",
      "20161221\n",
      "20161222\n",
      "20161223\n",
      "20161224\n",
      "20161225\n",
      "20161226\n",
      "20161227\n",
      "20161228\n",
      "20161229\n",
      "20161230\n",
      "20161231\n",
      "20170101\n",
      "20170102\n",
      "20170103\n",
      "20170104\n",
      "20170105\n",
      "20170106\n",
      "20170107\n",
      "20170108\n",
      "20170109\n",
      "20170110\n",
      "20170111\n",
      "20170112\n",
      "20170113\n",
      "20170114\n",
      "20170115\n",
      "20170116\n",
      "20170117\n",
      "20170118\n",
      "20170119\n",
      "20170120\n",
      "20170121\n",
      "20170122\n",
      "20170123\n",
      "20170124\n",
      "20170125\n",
      "20170126\n",
      "20170127\n",
      "20170128\n",
      "20170129\n",
      "20170130\n",
      "20170131\n",
      "20170201\n",
      "20170202\n",
      "20170203\n",
      "20170204\n",
      "20170205\n",
      "20170206\n",
      "20170207\n",
      "20170208\n",
      "20170209\n",
      "20170210\n",
      "20170211\n",
      "20170212\n",
      "20170213\n",
      "20170214\n",
      "20170215\n",
      "20170216\n",
      "20170217\n",
      "20170218\n",
      "20170219\n",
      "20170220\n",
      "20170221\n",
      "20170222\n",
      "20170223\n",
      "20170224\n",
      "20170225\n",
      "20170226\n",
      "20170227\n",
      "20170228\n",
      "20170301\n",
      "20170302\n",
      "20170303\n",
      "20170304\n",
      "20170305\n",
      "20170306\n",
      "20170307\n",
      "20170308\n",
      "20170309\n",
      "20170310\n",
      "20170311\n",
      "20170312\n",
      "20170313\n",
      "20170314\n",
      "20170315\n",
      "20170316\n",
      "20170317\n",
      "20170318\n",
      "20170319\n",
      "20170320\n",
      "20170321\n",
      "20170322\n",
      "20170323\n",
      "20170324\n",
      "20170325\n",
      "20170326\n",
      "20170327\n",
      "20170328\n",
      "20170329\n",
      "20170330\n",
      "20170331\n",
      "20170401\n",
      "20170402\n",
      "20170403\n",
      "20170404\n",
      "20170405\n",
      "20170406\n",
      "20170407\n",
      "20170408\n",
      "20170409\n",
      "20170410\n",
      "20170411\n",
      "20170412\n",
      "\n",
      "Scraping salary information from the 2017-18 regular season\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57630a95f51c4d8bbb6a99f2cbd75a7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=177), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20171017\n",
      "20171018\n",
      "20171019\n",
      "20171020\n",
      "20171021\n",
      "20171022\n",
      "20171023\n",
      "20171024\n",
      "20171025\n",
      "20171026\n",
      "20171027\n",
      "20171028\n",
      "20171029\n",
      "20171030\n",
      "20171031\n",
      "20171101\n",
      "20171102\n",
      "20171103\n",
      "20171104\n",
      "20171105\n",
      "20171106\n",
      "20171107\n",
      "20171108\n",
      "20171109\n",
      "20171110\n",
      "20171111\n",
      "20171112\n",
      "20171113\n",
      "20171114\n",
      "20171115\n",
      "20171116\n",
      "20171117\n",
      "20171118\n",
      "20171119\n",
      "20171120\n",
      "20171121\n",
      "20171122\n",
      "20171123\n",
      "20171124\n",
      "20171125\n",
      "20171126\n",
      "20171127\n",
      "20171128\n",
      "20171129\n",
      "20171130\n",
      "20171201\n",
      "20171202\n",
      "20171203\n",
      "20171204\n",
      "20171205\n",
      "20171206\n",
      "20171207\n",
      "20171208\n",
      "20171209\n",
      "20171210\n",
      "20171211\n",
      "20171212\n",
      "20171213\n",
      "20171214\n",
      "20171215\n",
      "20171216\n",
      "20171217\n",
      "20171218\n",
      "20171219\n",
      "20171220\n",
      "20171221\n",
      "20171222\n",
      "20171223\n",
      "20171224\n",
      "20171225\n",
      "20171226\n",
      "20171227\n",
      "20171228\n",
      "20171229\n",
      "20171230\n",
      "20171231\n",
      "20180101\n",
      "20180102\n",
      "20180103\n",
      "20180104\n",
      "20180105\n",
      "20180106\n",
      "20180107\n",
      "20180108\n",
      "20180109\n",
      "20180110\n",
      "20180111\n",
      "20180112\n",
      "20180113\n",
      "20180114\n",
      "20180115\n",
      "20180116\n",
      "20180117\n",
      "20180118\n",
      "20180119\n",
      "20180120\n",
      "20180121\n",
      "20180122\n",
      "20180123\n",
      "20180124\n",
      "20180125\n",
      "20180126\n",
      "20180127\n",
      "20180128\n",
      "20180129\n",
      "20180130\n",
      "20180131\n",
      "20180201\n",
      "20180202\n",
      "20180203\n",
      "20180204\n",
      "20180205\n",
      "20180206\n",
      "20180207\n",
      "20180208\n",
      "20180209\n",
      "20180210\n",
      "20180211\n",
      "20180212\n",
      "20180213\n",
      "20180214\n",
      "20180215\n",
      "20180216\n",
      "20180217\n",
      "20180218\n",
      "20180219\n",
      "20180220\n",
      "20180221\n",
      "20180222\n",
      "20180223\n",
      "20180224\n",
      "20180225\n",
      "20180226\n",
      "20180227\n",
      "20180228\n",
      "20180301\n",
      "20180302\n",
      "20180303\n",
      "20180304\n",
      "20180305\n",
      "20180306\n",
      "20180307\n",
      "20180308\n",
      "20180309\n",
      "20180310\n",
      "20180311\n",
      "20180312\n",
      "20180313\n",
      "20180314\n",
      "20180315\n",
      "20180316\n",
      "20180317\n",
      "20180318\n",
      "20180319\n",
      "20180320\n",
      "20180321\n",
      "20180322\n",
      "20180323\n",
      "20180324\n",
      "20180325\n",
      "20180326\n",
      "20180327\n",
      "20180328\n",
      "20180329\n",
      "20180330\n",
      "20180331\n",
      "20180401\n",
      "20180402\n",
      "20180403\n",
      "20180404\n",
      "20180405\n",
      "20180406\n",
      "20180407\n",
      "20180408\n",
      "20180409\n",
      "20180410\n",
      "20180411\n",
      "\n",
      "Scraping salary information from the 2018-19 regular season\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6492e86a19fc4ce0bfdbe27df96e5951",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=177), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20181016\n",
      "20181017\n",
      "20181018\n",
      "20181019\n",
      "20181020\n",
      "20181021\n",
      "20181022\n",
      "20181023\n",
      "20181024\n",
      "20181025\n",
      "20181026\n",
      "20181027\n",
      "20181028\n",
      "20181029\n",
      "20181030\n",
      "20181031\n",
      "20181101\n",
      "20181102\n",
      "20181103\n",
      "20181104\n",
      "20181105\n",
      "20181106\n",
      "20181107\n",
      "20181108\n",
      "20181109\n",
      "20181110\n",
      "20181111\n",
      "20181112\n",
      "20181113\n",
      "20181114\n",
      "20181115\n",
      "20181116\n",
      "20181117\n",
      "20181118\n",
      "20181119\n",
      "20181120\n",
      "20181121\n",
      "20181122\n",
      "20181123\n",
      "20181124\n",
      "20181125\n",
      "20181126\n",
      "20181127\n",
      "20181128\n",
      "20181129\n",
      "20181130\n",
      "20181201\n",
      "20181202\n",
      "20181203\n",
      "20181204\n",
      "20181205\n",
      "20181206\n",
      "20181207\n",
      "20181208\n",
      "20181209\n",
      "20181210\n",
      "20181211\n",
      "20181212\n",
      "20181213\n",
      "20181214\n",
      "20181215\n",
      "20181216\n",
      "20181217\n",
      "20181218\n",
      "20181219\n",
      "20181220\n",
      "20181221\n",
      "20181222\n",
      "20181223\n",
      "20181224\n",
      "20181225\n",
      "20181226\n",
      "20181227\n",
      "20181228\n",
      "20181229\n",
      "20181230\n",
      "20181231\n",
      "20190101\n",
      "20190102\n",
      "20190103\n",
      "20190104\n",
      "20190105\n",
      "20190106\n",
      "20190107\n",
      "20190108\n",
      "20190109\n",
      "20190110\n",
      "20190111\n",
      "20190112\n",
      "20190113\n",
      "20190114\n",
      "20190115\n",
      "20190116\n",
      "20190117\n",
      "20190118\n",
      "20190119\n",
      "20190120\n",
      "20190121\n",
      "20190122\n",
      "20190123\n",
      "20190124\n",
      "20190125\n",
      "20190126\n",
      "20190127\n",
      "20190128\n",
      "20190129\n",
      "20190130\n",
      "20190131\n",
      "20190201\n",
      "20190202\n",
      "20190203\n",
      "20190204\n",
      "20190205\n",
      "20190206\n",
      "20190207\n",
      "20190208\n",
      "20190209\n",
      "20190210\n",
      "20190211\n",
      "20190212\n",
      "20190213\n",
      "20190214\n",
      "20190215\n",
      "20190216\n",
      "20190217\n",
      "20190218\n",
      "20190219\n",
      "20190220\n",
      "20190221\n",
      "20190222\n",
      "20190223\n",
      "20190224\n",
      "20190225\n",
      "20190226\n",
      "20190227\n",
      "20190228\n",
      "20190301\n",
      "20190302\n",
      "20190303\n",
      "20190304\n",
      "20190305\n",
      "20190306\n",
      "20190307\n",
      "20190308\n",
      "20190309\n",
      "20190310\n",
      "20190311\n",
      "20190312\n",
      "20190313\n",
      "20190314\n",
      "20190315\n",
      "20190316\n",
      "20190317\n",
      "20190318\n",
      "20190319\n",
      "20190320\n",
      "20190321\n",
      "20190322\n",
      "20190323\n",
      "20190324\n",
      "20190325\n",
      "20190326\n",
      "20190327\n",
      "20190328\n",
      "20190329\n",
      "20190330\n",
      "20190331\n",
      "20190401\n",
      "20190402\n",
      "20190403\n",
      "20190404\n",
      "20190405\n",
      "20190406\n",
      "20190407\n",
      "20190408\n",
      "20190409\n",
      "20190410\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scraper = DataScraper()\n",
    "\n",
    "# Comment out season dates in SEASON_DATES in constants.py to extract data for specific seasons\n",
    "for data_type in ['Boxscores', 'DKSalary']:\n",
    "    for season in SEASON_DATES.keys(): \n",
    "        if not os.path.exists(os.path.join(DATA_DIR, data_type, season)):\n",
    "            # Create a new directory and scrape the entire season\n",
    "            os.mkdir(os.path.join(DATA_DIR, data_type, season))\n",
    "            start_date = SEASON_DATES[season][0]\n",
    "            end_date = SEASON_DATES[season][1]\n",
    "            date_list = [d.strftime('%Y%m%d') for d in pd.date_range(start_date, end_date)]\n",
    "\n",
    "            if data_type == 'Boxscores':\n",
    "                scraper.get_boxscores(season, date_list)\n",
    "            else:\n",
    "                scraper.get_fantasy_salary(season, date_list)\n",
    "\n",
    "\n",
    "        elif os.path.exists(os.path.join(DATA_DIR, data_type, season)):\n",
    "            # Iterate over the existing files by name and scrape missing dates\n",
    "            start_date = SEASON_DATES[season][0]\n",
    "            end_date = SEASON_DATES[season][1]\n",
    "            # Dates to scrape box scores from\n",
    "            date_list = [d.strftime('%Y%m%d') for d in pd.date_range(start_date, end_date)]\n",
    "                            \n",
    "            if data_type == 'Boxscores':               \n",
    "                for date in date_list:\n",
    "                    # Check if csv files of the form {date}-{hometeam}.csv (i.e. 20131029-CHI.csv) exists\n",
    "                    if len(glob.glob(os.path.join(DATA_DIR, data_type, season, str(date)+\"*.csv\"))) > 0:\n",
    "                        # Set back the start day by \n",
    "                        date_list = date_list[date_list.index(date):]\n",
    "\n",
    "                scraper.get_boxscores(season, date_list)\n",
    "                \n",
    "            else:\n",
    "                for date in date_list:\n",
    "                    # Check if csv files of the form salary_{date}.csv (i.e. salary_20131029.csv) exists\n",
    "                    if os.path.exists(os.path.join(DATA_DIR, data_type, season, \"salary_{}.csv\".format(date))):\n",
    "                        date_list = date_list[date_list.index(date):]\n",
    "\n",
    "                scraper.get_fantasy_salary(season, date_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
